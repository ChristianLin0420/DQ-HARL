# Q-DAPPO Configuration - extends DAPPO with Q-weighted optimization
seed:
  # whether to use the specified seed
  seed_specify: True
  # seed
  seed: 1
  # torch_threads: 4
  # cudnn_deterministic: True

device:
  # whether to use CUDA
  cuda: True
  # whether to set CUDA deterministic
  cuda_deterministic: True
  # arg to torch.set_num_threads
  torch_threads: 4

train:
  # number of parallel environments for training data collection (reduced for memory)
  n_rollout_threads: 8
  # number of total training steps
  num_env_steps: 10000000
  # number of steps per environment per training data collection (reduced for memory)
  episode_length: 100
  # logging interval
  log_interval: 5
  # evaluation interval
  eval_interval: 25
  # whether to use ValueNorm
  use_valuenorm: True
  # whether to use linear learning rate decay (disabled for adaptive LR in Q-DAPPO)
  use_linear_lr_decay: False
  # whether to consider the case of truncation when an episode is done
  use_proper_time_limits: True
  # if set, load models from this directory; otherwise, randomly initialise the models
  model_dir: ~

eval:
  # whether to use evaluation
  use_eval: True
  # number of parallel environments for evaluation (reduced for memory)
  n_eval_rollout_threads: 4
  # number of episodes per evaluation (reduced for memory)
  eval_episodes: 10

render:
  # whether to use render
  use_render: False
  # number of episodes to render
  render_episodes: 10

model:
  # network parameters
  # hidden sizes for mlp module in the network
  hidden_sizes: [128, 128]
  # activation function, choose from sigmoid, tanh, relu, leaky_relu, silu, selu
  activation_func: gelu
  # whether to use feature normalization
  use_feature_normalization: True
  # initialization method for network parameters, choose from xavier_uniform_, orthogonal_, ...
  initialization_method: orthogonal_
  # gain of the output layer of the network
  gain: 0.01
  # recurrent parameters
  # whether to use rnn policy (data is not chunked for training)
  use_naive_recurrent_policy: False
  # whether to use rnn policy (data is chunked for training)
  use_recurrent_policy: False
  # number of recurrent layers
  recurrent_n: 1
  # length of data chunk; only useful when use_recurrent_policy is True; episode_length has to be a multiple of data_chunk_length
  data_chunk_length: 10
  # optimizer parameters
  # actor learning rate
  lr: 0.0001
  # critic learning rate
  critic_lr: 0.0005
  # eps in Adam
  opti_eps: 0.00001
  # weight_decay in Adam
  weight_decay: 0
  # parameters of diagonal Gaussian distribution
  std_x_coef: 1
  # parameters of diagonal Gaussian distribution
  std_y_coef: 0.5
  # diffusion model specific parameters
  # time embedding dimension for diffusion process
  time_embed_dim: 32

optim:
  # learning rate for actor
  lr: 0.0005
  # learning rate for critic
  critic_lr: 0.0005
  # eps for Adam optimizer
  opti_eps: 0.00001
  # weight decay for Adam optimizer
  weight_decay: 0

algo:
  # ppo parameters
  # number of epochs for actor update (reduced for memory)
  ppo_epoch: 2
  # number of epochs for critic update (reduced for memory)
  critic_epoch: 2
  # whether to use clipped value loss
  use_clipped_value_loss: True
  # clip parameter
  clip_param: 0.05
  # number of mini-batches per epoch for actor update
  actor_num_mini_batch: 1
  # number of mini-batches per epoch for critic update
  critic_num_mini_batch: 1
  # coefficient for entropy term in actor loss
  entropy_coef: 0.05
  # coefficient for value loss
  value_loss_coef: 1
  # whether to clip gradient norm
  use_max_grad_norm: True
  # max gradient norm
  max_grad_norm: 5.0
  # whether to use Generalized Advantage Estimation (GAE)
  use_gae: True
  # discount factor
  gamma: 0.99
  # GAE lambda
  gae_lambda: 0.95
  # whether to use huber loss
  use_huber_loss: True
  # whether to use policy active masks
  use_policy_active_masks: True
  # huber delta
  huber_delta: 10.0
  # method of aggregating the probability of multi-dimensional actions, choose from prod, mean
  action_aggregation: sum
  # whether to share parameter among actors
  share_param: True
  # whether to use a fixed optimisation order
  fixed_order: False
  
  # Q-DAPPO specific parameters
  # Q-weight transformation method
  q_weight_type: "softmax"  # softmax, exp, sigmoid, advantage
  # Temperature for Q-weight transformation
  q_temperature: 1.0
  # Coefficient for Q-weighted loss
  q_weight_coef: 1.0
  # Coefficient for diffusion entropy regularization
  diffusion_entropy_coef: 0.01
  # Whether to clip Q-values
  use_q_clipping: True
  # Q-value clipping range
  q_clip_range: [-10.0, 10.0]
  
  # Sample generation parameters (Algorithm 1 from QVPO)
  # Number of samples from diffusion policy (Nd) - reduced for memory efficiency
  num_diffusion_samples: 4
  # Number of samples from uniform distribution (Ne) - reduced for memory efficiency  
  num_uniform_samples: 2
  # Method for action selection
  sample_selection_method: "max_weight"  # max_weight, weighted_sampling
  
  # Enhanced entropy regularization
  use_diffusion_entropy: True
  entropy_estimation_method: "sample_based"  # gaussian_approx, sample_based
  
  # Original DAPPO/GenPO parameters
  # coefficient for entropy loss with importance sampling
  lambda_ent: 0.01
  # coefficient for compression loss
  nu_compression: 0.01
  # whether to enable adaptive learning rate
  adaptive_lr: True
  # KL threshold to decrease learning rate
  kl_threshold_high: 0.02
  # KL threshold to increase learning rate
  kl_threshold_low: 0.005
  # factor for learning rate adjustment
  lr_adjustment_factor: 2.0
  
  # diffusion parameters
  # number of diffusion timesteps (reduced for memory efficiency)
  num_diffusion_steps: 5
  # mixing probability for coupled noise vectors
  mixing_p: 0.9
  # start value for noise schedule
  beta_start: 0.0001
  # end value for noise schedule
  beta_end: 0.02
  
  # Q-critic specific parameters
  # polyak averaging coefficient for target network updates
  polyak: 0.005

logger:
  # logging directory
  log_dir: "./results" 